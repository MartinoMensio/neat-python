"""Extension for DeepNEAT"""
from neat import genes
from neat.attributes import FloatAttribute, BoolAttribute, StringAttribute

# for each node, the 'len_shape' tells how many dimensions each input and output have.
# 1 --> (hidden), 2 --> (word, hidden)
_node_descriptions = {
    # the input node
    'embeddings': {
        'outputs': {
            'out_0': {
                'len_shape': 2
            }
        }
    },
    # the output nodes
    'out_seq': {
        'inputs': {
            'in_0': {
                'len_shape': 2
            }
        }
    },
    'out_single': {
        'inputs': {
            'in_0': {
                'len_shape': 1
            }
        }
    },
    # the mutating intermediate nodes
    'dense': {
        'inputs': {
            'in_0': {
                'len_shape': 1
            }
        },
        'outputs': {
            'out_0': {
                'len_shape': 1
            }
        }
    },
    'rnn_monodir': {
        'inputs': {
            'in_0': {
                'len_shape': 2
            }
        },
        'outputs': {
            'out_final': {
                'len_shape': 1
            },
            'out_steps': {
                'len_shape': 2
            }
        }
    },
    'rnn_bidir': {
        'inputs': {
            'in_0': {
                'len_shape': 2
            }
        },
        'outputs': {
            'out_final': {
                'len_shape': 1
            },
            'out_steps': {
                'len_shape': 2
            }
        }
    },
    'decoder': {
        'inputs': {
            'in_0': {
                'len_shape': 2
            }
        },
        'outputs': {
            'out_0': {
                'len_shape': 2
            }
        }
    },
    'decoder_att': {
        'inputs': {
            'in_0': {
                'len_shape': 2
            }
        },
        'outputs': {
            'out_0': {
                'len_shape': 2
            }
        }
    },
    'yang_att': {
        'inputs': {
            'in_0': {
                'len_shape': 2
            }
        },
        'outputs': {
            'out_0': {
                'len_shape': 1
            }
        }
    }
}

class DefaultNodeGene_deep(genes.DefaultNodeGene):
    # here are enumerated the types of nodes that can be generated by the GA. The initial configurations can also have other types such as in, out, embeddings
    # concat is not a layer, but simply how to merge links
    _gene_attributes = [StringAttribute('type_of_layer', options='dense rnn_monodir rnn_bidir decoder decoder_att yang_att'),
                        #StringAttribute('num_of_nodes', options='512 1024 2048'), # maybe later
                        #StringAttribute('activation', options='relu sigmoid'),
                        #StringAttribute('num_filters_conv', options='8 16 32 64'),
                        #StringAttribute('kernel_size_conv', options='1 3 5 7'),
                        #StringAttribute('stride_conv', options='1 2'),
                        #StringAttribute('stride_pool', options='1 2'),
                        #StringAttribute('poolsize_pool', options='2 3'),
                        #StringAttribute('has_maxpool', options='true false')
                        #StringAttribute('hidden_size', options='16 32 64 128 256')]
                        # TODO discover why the option values get destroyed (options from last attribute overwrite other attributes)
    ]

    def __init__(self, key):
        genes.DefaultNodeGene.__init__(self, key)
        #print(key)
        type_of_layer = key.split('.')[0]
        self.description = _node_descriptions[type_of_layer]
        #exit(1)

    def distance(self, other, config):
        factors = {
            'type_of_layer': 10,
            'hidden_size': 2,
            'activation': 1
        }
        

        d = 0.0
        if self.type_of_layer != other.type_of_layer:
            d += factors['type_of_layer']
        else:
            if self.type_of_layer == 'recurrent':
                pass
                #d += (abs(float(self.hidden_size)-float(other.hidden_size))/128)*factors['hidden_size']
            # TODO consider other differences

        return d * config.compatibility_weight_coefficient
        
class DefaultConnectionGene_deep(genes.DefaultConnectionGene):
    _gene_attributes = [BoolAttribute('enabled')]

    def distance(self, other, config):
        d = 0.0
        if self.enabled != other.enabled:
            d += 1.0
        return d * config.compatibility_weight_coefficient